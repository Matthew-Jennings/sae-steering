{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# An investigation into use of SAEs as steering vectors"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### If in Colab, install deps. Otherwise, setup autoreload."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    import google.colab\n",
    "\n",
    "    IN_COLAB = True\n",
    "    %pip install sae-lens transformer-lens\n",
    "\n",
    "except ImportError:\n",
    "    # Local\n",
    "    IN_COLAB = False\n",
    "\n",
    "    import IPython\n",
    "\n",
    "    ipython = IPython.get_ipython()\n",
    "    ipython.run_line_magic(\"load_ext\", \"autoreload\")\n",
    "    ipython.run_line_magic(\"autoreload\", \"2\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import collections\n",
    "import functools\n",
    "import json\n",
    "import math\n",
    "import os\n",
    "import requests\n",
    "from pprint import pprint\n",
    "import pathlib\n",
    "\n",
    "import huggingface_hub\n",
    "from matplotlib import pyplot as plt\n",
    "import pandas as pd\n",
    "from sae_lens import SAE\n",
    "from tqdm import tqdm, trange\n",
    "import torch as t\n",
    "from transformer_lens import HookedTransformer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Other settings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "t.set_grad_enabled(False)\n",
    "\n",
    "if t.backends.mps.is_available():\n",
    "    DEVICE = \"mps\"\n",
    "else:\n",
    "    DEVICE = \"cuda\" if t.cuda.is_available() else \"cpu\"\n",
    "\n",
    "print(f\"\\nDevice: {DEVICE}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### HuggingFace Login (for Gemma)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "huggingface_hub.notebook_login()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load GPT-2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gpt2_small = HookedTransformer.from_pretrained(\n",
    "    \"gpt2-small\",\n",
    "    device=DEVICE,\n",
    ")\n",
    "\n",
    "pprint(gpt2_small.cfg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import GPT-2 J-B SAEs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "saes = [\n",
    "    SAE.from_pretrained(\n",
    "        release=\"gpt2-small-res-jb\",\n",
    "        sae_id=f\"blocks.{layer}.hook_resid_pre\",\n",
    "        device=DEVICE,\n",
    "    )[0]\n",
    "    for layer in trange(gpt2_small.cfg.n_layers)\n",
    "]\n",
    "\n",
    "pprint(saes[0].cfg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Export SAE feature explanations for later search"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_explanations_from_saes(saes, save_path):\n",
    "    try:\n",
    "        with open(save_path, \"r\") as f:\n",
    "            explanations = json.load(f)\n",
    "\n",
    "    except FileNotFoundError:\n",
    "        url = \"https://www.neuronpedia.org/api/explanation/export\"\n",
    "\n",
    "        explanations = []\n",
    "\n",
    "        for i, sae in enumerate(tqdm(saes)):\n",
    "            model, sae_id = sae.cfg.neuronpedia_id.split(\"/\")\n",
    "\n",
    "            querystring = {\"modelId\": model, \"saeId\": sae_id}\n",
    "            headers = {\"X-Api-Key\": os.getenv(\"NEURONPEDIA_TOKEN\")}\n",
    "\n",
    "            response = requests.get(url, headers=headers, params=querystring)\n",
    "            explanations += response.json()\n",
    "\n",
    "            with open(save_path, \"w\") as f:\n",
    "                json.dump(explanations, f, indent=2)\n",
    "\n",
    "    return explanations, save_path\n",
    "\n",
    "\n",
    "explanations_fpath = \"gpt2-small_res-jb_explanations.json\"\n",
    "explanations, _ = load_explanations_from_saes(saes, explanations_fpath)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Find all features whose explanations contain keywords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_explanations_with_keywords_by_layer(explanations, keywords):\n",
    "    explanations_filtered = collections.defaultdict(list)\n",
    "\n",
    "    for explanation in explanations:\n",
    "        if any(keyword in explanation[\"description\"].upper() for keyword in keywords):\n",
    "            layer = int(explanation[\"layer\"].split(\"-\")[0])\n",
    "            explanations_filtered[layer].append(explanation)\n",
    "\n",
    "    return explanations_filtered\n",
    "\n",
    "\n",
    "keywords = [\"AUSTRALIA\"]\n",
    "\n",
    "explanations_filtered = get_explanations_with_keywords_by_layer(explanations, keywords)\n",
    "\n",
    "explanation_count = 0\n",
    "explanation_counts_by_layer = {}\n",
    "for layer in range(len(saes)):\n",
    "    explanation_count_in_layer = len(explanations_filtered[layer])\n",
    "    explanation_counts_by_layer[layer] = explanation_count_in_layer\n",
    "    explanation_count += explanation_count_in_layer\n",
    "    print(\n",
    "        f\"\\nNumber of relevant features in layer {layer}: {explanation_count_in_layer}\"\n",
    "    )\n",
    "    explanations_filtered_layer_str = \"\\n\\t\".join(\n",
    "        [\n",
    "            f\"{explanation['index']}:\\t{explanation['description']}\"\n",
    "            for explanation in explanations_filtered[layer]\n",
    "        ]\n",
    "    )\n",
    "    print(f\"\\t{explanations_filtered_layer_str}\")\n",
    "print(\n",
    "    f\"Total relevant features: {explanation_count} ({explanation_count / len(explanations):.6f}% of total)\"\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plt.figure(figsize=(10, 5))\n",
    "\n",
    "# creating the bar plot\n",
    "plt.bar(\n",
    "    explanation_counts_by_layer.keys(),\n",
    "    explanation_counts_by_layer.values(),\n",
    "    color=\"blue\",\n",
    "    width=0.4,\n",
    ")\n",
    "\n",
    "plt.xlabel(\"SAE/Layer #\")\n",
    "plt.xticks([l for l in explanation_counts_by_layer.keys()])\n",
    "plt.ylabel(f\"No. of features containing 'Australia'\")\n",
    "plt.title(\"No. of features containing 'Australia' by SAE/Layer\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Find SAE feature indices that correlate with intended steering direction "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_model_and_get_filtered_activations(\n",
    "    model, prompt, explanations_filtered, activation_threshold, quiet=True\n",
    "):\n",
    "    _, cache = model.run_with_cache(prompt, prepend_bos=True)\n",
    "\n",
    "    if not quiet:\n",
    "        tokens = model.to_tokens(prompt)\n",
    "        print(f\"Tokens: {tokens}\")\n",
    "        print(f\"Token strings: {model.to_str_tokens(tokens)}\")\n",
    "\n",
    "    saes_out = {}\n",
    "    acts_filtered = {}\n",
    "\n",
    "    for layer, sae in enumerate(tqdm(saes)):\n",
    "        if explanations_filtered[layer] == []:\n",
    "            continue\n",
    "\n",
    "        explanations_filtered_idx = t.tensor(\n",
    "            [int(explanation[\"index\"]) for explanation in explanations_filtered[layer]],\n",
    "            device=DEVICE,\n",
    "        )\n",
    "\n",
    "        feature_acts = sae.encode(\n",
    "            cache[sae.cfg.hook_name]\n",
    "        )  # shape (batch, sequence, features)\n",
    "\n",
    "        saes_out[layer] = sae.decode(feature_acts)\n",
    "\n",
    "        feature_acts_vals_sorted, idx = feature_acts[\n",
    "            :, :, explanations_filtered_idx\n",
    "        ].sort(descending=True, dim=-1)\n",
    "\n",
    "        feature_acts_idx_sorted = explanations_filtered_idx[idx]\n",
    "\n",
    "        mask = feature_acts_vals_sorted >= activation_threshold\n",
    "\n",
    "        acts_filtered[layer] = {\n",
    "            \"val\": feature_acts_vals_sorted[mask].tolist(),\n",
    "            \"idx\": feature_acts_idx_sorted[mask].tolist(),\n",
    "            \"desc\": [],\n",
    "        }\n",
    "\n",
    "        for idx in acts_filtered[layer][\"idx\"]:\n",
    "            acts_filtered[layer][\"desc\"].append(\n",
    "                next(\n",
    "                    expl[\"description\"]\n",
    "                    for expl in explanations_filtered[layer]\n",
    "                    if int(expl[\"index\"]) == idx\n",
    "                )\n",
    "            )\n",
    "\n",
    "    return (\n",
    "        cache,\n",
    "        acts_filtered,\n",
    "        saes_out,\n",
    "    )\n",
    "\n",
    "\n",
    "sv_prompt = \"Sydney Opera House\"\n",
    "activation_threshold = 1.0\n",
    "\n",
    "cache, acts_filtered, saes_out = run_model_and_get_filtered_activations(\n",
    "    gpt2_small, sv_prompt, explanations_filtered, activation_threshold, quiet=True\n",
    ")\n",
    "\n",
    "# print_relevant_features(act_vals, act_idx, gpt2_small)\n",
    "\n",
    "print(\"\\nFiltered activations:\")\n",
    "pprint(acts_filtered)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "steering_vectors_positions_by_layer = {}\n",
    "for layer, acts_filt_at_layer in acts_filtered.items():\n",
    "    if len(acts_filt_at_layer[\"idx\"]) == 0:\n",
    "        continue\n",
    "\n",
    "    position = saes_out[layer].shape[1] - 1\n",
    "    idx = acts_filt_at_layer[\"idx\"][0]\n",
    "    val = acts_filt_at_layer[\"val\"][0]\n",
    "    steering_vector = saes[layer].W_dec[idx]\n",
    "    explanation = next(\n",
    "        expl[\"description\"]\n",
    "        for expl in explanations_filtered[layer]\n",
    "        if int(expl[\"index\"]) == idx\n",
    "    )\n",
    "\n",
    "    steering_vectors_positions_by_layer[layer] = {\n",
    "        \"idx\": idx,\n",
    "        \"pos\": position,\n",
    "        \"sv\": steering_vector,\n",
    "        \"val\": val,\n",
    "        \"expl\": explanation,\n",
    "    }\n",
    "svps_by_layer_to_print = {\n",
    "    layer: {\n",
    "        \"idx\": svp[\"idx\"],\n",
    "        \"val\": svp[\"val\"],\n",
    "        \"expl\": svp[\"expl\"],\n",
    "    }\n",
    "    for layer, svp in steering_vectors_positions_by_layer.items()\n",
    "}\n",
    "print(f\"Total number of steering vectors: {len(steering_vectors_positions_by_layer)}\\n\")\n",
    "print(f\"Steering vector layers: {[k for k in steering_vectors_positions_by_layer]}\\n\")\n",
    "\n",
    "for layer, svp in steering_vectors_positions_by_layer.items():\n",
    "    print(\n",
    "        f\"Layer {layer}:\\n\\tIndex:\\t\\t{svp['idx']}\\n\\tValue:\\t\\t{svp['val']}\\n\\tExplanation:\\t{svp['expl']}\\n\"\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def steering_hook_all_layers(\n",
    "    resid_pre, hook, steering_on, steering_vector, coeff, position\n",
    "):\n",
    "    # position = sae_out.shape[1]\n",
    "    if steering_on:\n",
    "        resid_pre[:, : position - 1, :] += coeff * steering_vector\n",
    "\n",
    "\n",
    "def hooked_generate(\n",
    "    prompt_batch, model, fwd_hooks=[], max_new_tokens=20, seed=None, **kwargs\n",
    "):\n",
    "    if seed is not None:\n",
    "        t.manual_seed(seed)\n",
    "\n",
    "    with model.hooks(fwd_hooks=fwd_hooks):\n",
    "        tokenized = model.to_tokens(prompt_batch)\n",
    "        result = model.generate(\n",
    "            stop_at_eos=False,  # avoids a bug on MPS\n",
    "            input=tokenized,\n",
    "            max_new_tokens=max_new_tokens,\n",
    "            do_sample=True,\n",
    "            **kwargs,\n",
    "        )\n",
    "    return result\n",
    "\n",
    "\n",
    "def generate_multi_layer(\n",
    "    prompt: str,\n",
    "    model: HookedTransformer,\n",
    "    steering_vectors_positions_by_layer: dict,\n",
    "    coeff: float,\n",
    "    sampling_kwargs,\n",
    "    num_responses: int = 3,\n",
    "    steering_on: bool = True,\n",
    "    max_new_tokens: int = 20,\n",
    "):\n",
    "    model.reset_hooks()\n",
    "\n",
    "    editing_hooks = []\n",
    "    for layer, svp in steering_vectors_positions_by_layer.items():\n",
    "        temp_hook_fn = functools.partial(\n",
    "            steering_hook_all_layers,\n",
    "            steering_on=steering_on,\n",
    "            steering_vector=svp[\"sv\"],\n",
    "            coeff=coeff,\n",
    "            position=svp[\"pos\"],\n",
    "        )\n",
    "        editing_hooks.append((f\"blocks.{layer}.hook_resid_post\", temp_hook_fn))\n",
    "\n",
    "    res = hooked_generate(\n",
    "        [prompt] * num_responses,\n",
    "        model,\n",
    "        editing_hooks,\n",
    "        max_new_tokens,\n",
    "        seed=None,\n",
    "        **sampling_kwargs,\n",
    "    )\n",
    "\n",
    "    return res\n",
    "\n",
    "\n",
    "def print_res_str(res, model):\n",
    "    res_str = model.to_string(res[:, 1:])\n",
    "    for i in range(res.shape[0]):\n",
    "        print(f\"{res_str[i]}\\n\" + \"-\" * 80)\n",
    "\n",
    "\n",
    "def print_res_str_tokens(res, model):\n",
    "    for i in range(res.shape[0]):\n",
    "        res_str = model.to_str_tokens(res[i, 1:])\n",
    "        print(f\"{res_str}\\n\" + \"-\" * 80)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt = \"The White House is located in a country called\"\n",
    "coeff = 50\n",
    "\n",
    "sampling_kwargs = {\"temperature\": 1.0, \"top_p\": 0.1, \"freq_penalty\": 1.0}\n",
    "\n",
    "res = generate_multi_layer(\n",
    "    prompt=prompt,\n",
    "    model=gpt2_small,\n",
    "    steering_vectors_positions_by_layer=steering_vectors_positions_by_layer,\n",
    "    steering_on=False,\n",
    "    coeff=coeff,\n",
    "    sampling_kwargs=sampling_kwargs,\n",
    "    num_responses=3,\n",
    ")\n",
    "\n",
    "print(\"UNSTEERED:\")\n",
    "print_res_str(res, gpt2_small)\n",
    "\n",
    "res = generate_multi_layer(\n",
    "    prompt=prompt,\n",
    "    model=gpt2_small,\n",
    "    steering_vectors_positions_by_layer={2: steering_vectors_positions_by_layer[2]},\n",
    "    steering_on=True,\n",
    "    coeff=coeff,\n",
    "    sampling_kwargs=sampling_kwargs,\n",
    "    num_responses=3,\n",
    ")\n",
    "\n",
    "print(\"STEERED:\")\n",
    "print_res_str(res, gpt2_small)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# fmt: off\n",
    "landmark_country_pairs = [\n",
    "    [\"The Eiffel Tower\"         , \"France\"], # The unsteered model answers 'Switzerland' approx. 50% of the time\n",
    "    [\"The Taj Mahal\"            , \"Tajikistan\"], # Actually, India I think, but model baseline is this\n",
    "    [\"The Statue of Liberty\"    , \"The United States of America\"],\n",
    "    [\"The Colosseum\"            , \"Italy\"],\n",
    "    [\"The Great Pyramid of Giza\", \"Egypt\"],\n",
    "    [\"Stonehenge\"               , \"The United Kingdom\"], # Gives better results than England\n",
    "    [\"Petra\"                    , \"Jordan\"],\n",
    "    [\"Macchu Picchu\"            , \"Peru\"],\n",
    "    [\"Burj Khalifa\"             , \"Qatar\"], # Actually The United Arab Emirates\n",
    "    [\"The Kremlin\"              , \"Russia\"],\n",
    "]\n",
    "# fmt: on\n",
    "for i, (landmark, country) in enumerate(landmark_country_pairs):\n",
    "    prompt = f\"{landmark} is located in a country called\"\n",
    "    res = generate_multi_layer(\n",
    "        prompt=prompt,\n",
    "        model=gpt2_small,\n",
    "        steering_vectors_positions_by_layer=steering_vectors_positions_by_layer,\n",
    "        steering_on=False,\n",
    "        coeff=50,\n",
    "        num_responses=10,\n",
    "        sampling_kwargs=sampling_kwargs,\n",
    "        max_new_tokens=10,\n",
    "    )\n",
    "    print_res_str(res, gpt2_small)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_steering_of_landmark_location_in_model(\n",
    "    landmark_country_pairs,\n",
    "    model,\n",
    "    steering_on,\n",
    "    steering_vectors_positions_by_layer,\n",
    "    steered_answer,\n",
    "    coeff,\n",
    "    quiet=False,\n",
    "    strict=True,\n",
    "    max_new_tokens=10,\n",
    "):\n",
    "    # country_max_length = max(len(country) for _, country in landmark_country_pairs)\n",
    "\n",
    "    correct_counts = t.zeros(len(landmark_country_pairs), dtype=t.int)\n",
    "    steering_counts = t.zeros(len(landmark_country_pairs), dtype=t.int)\n",
    "\n",
    "    for i, (landmark, country) in enumerate(tqdm(landmark_country_pairs)):\n",
    "        # if country != \"Russia\":\n",
    "        #     continue\n",
    "        prompt = f\"{landmark} is located in a country called\"\n",
    "        res = generate_multi_layer(\n",
    "            prompt=prompt,\n",
    "            model=model,\n",
    "            steering_vectors_positions_by_layer=steering_vectors_positions_by_layer,\n",
    "            steering_on=steering_on,\n",
    "            coeff=coeff,\n",
    "            num_responses=100,\n",
    "            sampling_kwargs=sampling_kwargs,\n",
    "            max_new_tokens=max_new_tokens,\n",
    "        )\n",
    "        res_strs = gpt2_small.to_string(res[:, 1:])\n",
    "\n",
    "        for res_str in res_strs:\n",
    "            check_str = res_str[len(prompt) + 1 :].upper()\n",
    "            if strict:\n",
    "                if check_str.upper().startswith(country.upper()):\n",
    "                    correct_counts[i] += 1\n",
    "                if check_str.upper().startswith(steered_answer.upper()):\n",
    "                    steering_counts[i] += 1\n",
    "            else:\n",
    "                if country.upper() in check_str:\n",
    "                    correct_counts[i] += 1\n",
    "                if steered_answer.upper() in check_str:\n",
    "                    steering_counts[i] += 1\n",
    "\n",
    "        if not quiet:\n",
    "            print(\n",
    "                f\"{'STEERED' if steering_on else 'UNSTEERED'} test for landmark '{landmark}'\"\n",
    "            )\n",
    "            print(f\"Correct Answer: '{country}'\\tSteered Answer: 'Australia'\")\n",
    "            print(f\"\\tNumber of correct responses: {correct_counts[i]}\")\n",
    "            print(f\"\\tNumber of steered responses: {steering_counts[i]}\")\n",
    "\n",
    "    return correct_counts, steering_counts\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Unsteered results:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "correct_counts, steering_counts = test_steering_of_landmark_location_in_model(\n",
    "    model=gpt2_small,\n",
    "    landmark_country_pairs=landmark_country_pairs,\n",
    "    steering_on=False,\n",
    "    steering_vectors_positions_by_layer=steering_vectors_positions_by_layer,\n",
    "    steered_answer=\"Australia\",\n",
    "    coeff=coeff,\n",
    "    quiet=True,\n",
    "    strict=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_steering_results(landmark_country_pairs, correct_counts, steering_counts):\n",
    "    df = pd.DataFrame(\n",
    "        {\n",
    "            \"Landmarks\": [\n",
    "                landmark_country_pairs[i][0] for i in range(len(landmark_country_pairs))\n",
    "            ],\n",
    "            \"Expected Answers\": [\n",
    "                landmark_country_pairs[i][1] for i in range(len(landmark_country_pairs))\n",
    "            ],\n",
    "            \"Percent Correct\": correct_counts,\n",
    "            \"Percent Steered\": steering_counts,\n",
    "        }\n",
    "    )\n",
    "    df.style.format_index(str.upper, axis=1)\n",
    "    pd.set_option(\"display.width\", 200)\n",
    "\n",
    "    print(df.to_string(index=False))\n",
    "\n",
    "\n",
    "print_steering_results(landmark_country_pairs, correct_counts, steering_counts)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Steered"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "correct_counts, steering_counts = test_steering_of_landmark_location_in_model(\n",
    "    model=gpt2_small,\n",
    "    landmark_country_pairs=landmark_country_pairs,\n",
    "    steering_on=True,\n",
    "    steering_vectors_positions_by_layer={2: steering_vectors_positions_by_layer[2]},\n",
    "    steered_answer=\"Australia\",\n",
    "    coeff=50,\n",
    "    quiet=True,\n",
    ")\n",
    "\n",
    "print_steering_results(landmark_country_pairs, correct_counts, steering_counts)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Not strict"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 10 tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "correct_counts, steering_counts = test_steering_of_landmark_location_in_model(\n",
    "    model=gpt2_small,\n",
    "    landmark_country_pairs=landmark_country_pairs,\n",
    "    steering_on=True,\n",
    "    steering_vectors_positions_by_layer={2: steering_vectors_positions_by_layer[2]},\n",
    "    steered_answer=\"Australia\",\n",
    "    coeff=50,\n",
    "    quiet=True,\n",
    "    strict=False,\n",
    "    max_new_tokens=10,\n",
    ")\n",
    "\n",
    "print_steering_results(landmark_country_pairs, correct_counts, steering_counts)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 50 tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "correct_counts, steering_counts = test_steering_of_landmark_location_in_model(\n",
    "    model=gpt2_small,\n",
    "    landmark_country_pairs=landmark_country_pairs,\n",
    "    steering_on=True,\n",
    "    steering_vectors_positions_by_layer={2: steering_vectors_positions_by_layer[2]},\n",
    "    steered_answer=\"Australia\",\n",
    "    coeff=50,\n",
    "    quiet=True,\n",
    "    strict=False,\n",
    "    max_new_tokens=50,\n",
    ")\n",
    "\n",
    "print_steering_results(landmark_country_pairs, correct_counts, steering_counts)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Try a range of coeffs and steer using one feature at a time and compare"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "coeffs = [\n",
    "    0.0,\n",
    "    0.1,\n",
    "    0.2,\n",
    "    0.5,\n",
    "    1.0,\n",
    "    2.0,\n",
    "    5.0,\n",
    "    10.0,\n",
    "    20.0,\n",
    "    50.0,\n",
    "    100.0,\n",
    "    500.0,\n",
    "    1000.0,\n",
    "    5000.0,\n",
    "]\n",
    "\n",
    "correct_counts_all_strict = t.zeros(\n",
    "    (\n",
    "        len(steering_vectors_positions_by_layer),\n",
    "        len(coeffs),\n",
    "        len(landmark_country_pairs),\n",
    "    ),\n",
    "    dtype=t.int,\n",
    ")\n",
    "steering_counts_all_strict = correct_counts_all_strict.clone().detach()\n",
    "\n",
    "i = 0\n",
    "for layer, svp in tqdm(steering_vectors_positions_by_layer.items()):\n",
    "    for c, coeff in enumerate(coeffs):\n",
    "        steering_on = False if math.isclose(coeff, 0.0) else True\n",
    "\n",
    "        correct_counts_all_strict[i][c], steering_counts_all_strict[i][c] = (\n",
    "            test_steering_of_landmark_location_in_model(\n",
    "                model=gpt2_small,\n",
    "                landmark_country_pairs=landmark_country_pairs,\n",
    "                steering_on=steering_on,\n",
    "                steering_vectors_positions_by_layer={layer: svp},\n",
    "                steered_answer=\"Australia\",\n",
    "                coeff=coeff,\n",
    "                quiet=True,\n",
    "            )\n",
    "        )\n",
    "    i += 1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"Coeffs: {coeffs}\")\n",
    "for i, key in enumerate(steering_vectors_positions_by_layer.keys()):\n",
    "    print(f\"\\nLayer {key}\")\n",
    "    print(f\"Feature Index:\\t\\t{steering_vectors_positions_by_layer[key]['idx']}\")\n",
    "    print(f\"Feature Act Val:\\t{steering_vectors_positions_by_layer[key]['val']}\")\n",
    "    print(f\"Feature Explanation:\\t{steering_vectors_positions_by_layer[key]['expl']}\")\n",
    "    print(\"Correct counts:\")\n",
    "    pprint(correct_counts_all_strict[i])\n",
    "    print(\"Steering counts:\")\n",
    "    pprint(steering_counts_all_strict[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "\n",
    "def create_steering_plot(counts, steering_coeffs, suptitle, subplot_titles):\n",
    "    fig, axs = plt.subplots(2, 3, figsize=(18, 10))\n",
    "    fig.suptitle(suptitle, fontsize=16)\n",
    "    axs_flat = axs.flatten()\n",
    "\n",
    "    num_pairs = counts.shape[2]\n",
    "    for i, ax in enumerate(axs_flat):\n",
    "        counts_layer = counts[i]\n",
    "\n",
    "        for j in range(num_pairs):\n",
    "            ax.plot(\n",
    "                steering_coeffs, counts_layer[:, j], label=landmark_country_pairs[j][1]\n",
    "            )\n",
    "            ax.set_xscale(\"log\")\n",
    "            ax.set_xlabel(\"Steering Coefficients\")\n",
    "            ax.set_ylabel(\"Values\")\n",
    "            ax.set_ylim([0, 105])\n",
    "            ax.set_title(subplot_titles[i])\n",
    "\n",
    "    handles, labels = axs_flat[-1].get_legend_handles_labels()\n",
    "    fig.legend(handles, labels, loc=\"center right\", bbox_to_anchor=(0.98, 0.5), ncol=1)\n",
    "    # fig.legend(axs_flat, labels=, loc=\"right\", bbox_to_anchor=(0.91, 0.5))\n",
    "\n",
    "    plt.tight_layout(rect=[0, 0, 0.82, 0.98])\n",
    "    plt.subplots_adjust(wspace=0.3, hspace=0.3)\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "create_steering_plot(\n",
    "    counts=correct_counts_all_strict,\n",
    "    steering_coeffs=coeffs,\n",
    "    suptitle=\"Steering Results (STRICT) - 'Correct' Answer Count with Increasing Steering Coefficient\",\n",
    "    subplot_titles=[\n",
    "        f\"Layer {k}, Feature Idx {svp['idx']}\"\n",
    "        for k, svp in steering_vectors_positions_by_layer.items()\n",
    "    ],\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "create_steering_plot(\n",
    "    counts=steering_counts_all_strict,\n",
    "    steering_coeffs=coeffs,\n",
    "    suptitle=\"Steering Results (STRICT)  - 'Steered' Answer Count with Increasing Steering Coefficient\",\n",
    "    subplot_titles=[\n",
    "        f\"Layer {k}, Feature Idx {svp['idx']}\"\n",
    "        for k, svp in steering_vectors_positions_by_layer.items()\n",
    "    ],\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Unstrict_10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "correct_counts_all_unstrict = t.zeros(\n",
    "    (\n",
    "        len(steering_vectors_positions_by_layer),\n",
    "        len(coeffs),\n",
    "        len(landmark_country_pairs),\n",
    "    ),\n",
    "    dtype=t.int,\n",
    ")\n",
    "steering_counts_all_unstrict = correct_counts_all_unstrict.clone().detach()\n",
    "\n",
    "i = 0\n",
    "for layer, svp in tqdm(steering_vectors_positions_by_layer.items()):\n",
    "    for c, coeff in enumerate(coeffs):\n",
    "        steering_on = False if math.isclose(coeff, 0.0) else True\n",
    "\n",
    "        correct_counts_all_unstrict[i][c], steering_counts_all_unstrict[i][c] = (\n",
    "            test_steering_of_landmark_location_in_model(\n",
    "                model=gpt2_small,\n",
    "                landmark_country_pairs=landmark_country_pairs,\n",
    "                steering_on=steering_on,\n",
    "                steering_vectors_positions_by_layer={layer: svp},\n",
    "                steered_answer=\"Australia\",\n",
    "                coeff=coeff,\n",
    "                quiet=True,\n",
    "                strict=False,\n",
    "                max_new_tokens=10,\n",
    "            )\n",
    "        )\n",
    "    i += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"Coeffs: {coeffs}\")\n",
    "for i, key in enumerate(steering_vectors_positions_by_layer.keys()):\n",
    "    print(f\"\\nLayer {key}\")\n",
    "    print(f\"Feature Index:\\t\\t{steering_vectors_positions_by_layer[key]['idx']}\")\n",
    "    print(f\"Feature Act Val:\\t{steering_vectors_positions_by_layer[key]['val']}\")\n",
    "    print(f\"Feature Explanation:\\t{steering_vectors_positions_by_layer[key]['expl']}\")\n",
    "    print(\"Correct counts:\")\n",
    "    pprint(correct_counts_all_unstrict[i])\n",
    "    print(\"Steering counts:\")\n",
    "    pprint(steering_counts_all_unstrict[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "create_steering_plot(\n",
    "    counts=correct_counts_all_unstrict,\n",
    "    steering_coeffs=coeffs,\n",
    "    suptitle=\"Steering Results (NOT STRICT) - 'Correct' Answer Count with Increasing Steering Coefficient\",\n",
    "    subplot_titles=[\n",
    "        f\"Layer {k}, Feature Idx {svp['idx']}\"\n",
    "        for k, svp in steering_vectors_positions_by_layer.items()\n",
    "    ],\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "create_steering_plot(\n",
    "    counts=steering_counts_all_unstrict,\n",
    "    steering_coeffs=coeffs,\n",
    "    suptitle=\"Steering Results (NOT STRICT) - 'Steered' Answer Count with Increasing Steering Coefficient\",\n",
    "    subplot_titles=[\n",
    "        f\"Layer {k}, Feature Idx {svp['idx']}\"\n",
    "        for k, svp in steering_vectors_positions_by_layer.items()\n",
    "    ],\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Strict steering results by landmark.\n",
    "for k in range(steering_counts_all_strict.shape[2]):\n",
    "    print(landmark_country_pairs[k])\n",
    "    print(coeffs)\n",
    "    print(steering_counts_all_strict[:, :, k])  # shape (steering vectors, coeffs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Strict steering results summed over landmarks.\n",
    "steering_counts_summed_over_landmarks_strict = steering_counts_all_strict.sum(\n",
    "    dim=2\n",
    ")  # shape (steering vectors, coeffs)\n",
    "steering_counts_summed_over_landmarks_coeffs_strict = (\n",
    "    steering_counts_summed_over_landmarks_strict.sum(dim=1)\n",
    ")\n",
    "\n",
    "print(steering_counts_summed_over_landmarks_strict)\n",
    "print(steering_counts_summed_over_landmarks_coeffs_strict)\n",
    "\n",
    "layers = list(steering_vectors_positions_by_layer.keys())\n",
    "\n",
    "\n",
    "def create_sum_plot(summed_counts, steering_coeffs, labels, suptitle):\n",
    "    fig, ax = plt.subplots(1, 1)\n",
    "    fig.suptitle(suptitle, fontsize=16)\n",
    "\n",
    "    for i in range(len(labels)):\n",
    "        ax.plot(steering_coeffs, summed_counts[i, :], label=labels[i])\n",
    "        ax.set_xscale(\"log\")\n",
    "        ax.set_xlabel(\"Steering Coefficients\")\n",
    "        ax.set_xlim([1, 5000])\n",
    "        ax.set_ylabel(\"Count\")\n",
    "\n",
    "    handles, labels = ax.get_legend_handles_labels()\n",
    "    fig.legend(handles, labels, loc=\"center right\", bbox_to_anchor=(1.2, 0.5), ncol=1)\n",
    "\n",
    "    plt.tight_layout(rect=[-0.2, 0, 0.8, 0.98])\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "def create_bar_plot(vals, labels, title, xlabel, ylabel, rotation=0):\n",
    "    fig, ax = plt.subplots(1, 1)\n",
    "    x_pos = np.arange(len(labels))\n",
    "    ax.bar(x_pos, vals, align=\"center\")\n",
    "    ax.set_title(title, fontsize=16, pad=20)\n",
    "    ax.set_xticks(np.arange(len(labels)), labels, rotation=rotation)\n",
    "    ax.set_xlabel(xlabel, labelpad=20)\n",
    "    ax.set_ylabel(ylabel)\n",
    "\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "create_sum_plot(\n",
    "    summed_counts=steering_counts_summed_over_landmarks_strict,\n",
    "    steering_coeffs=coeffs,\n",
    "    labels=[\n",
    "        f\"Layer {layer}, Feature Idx {svp['idx']}\"\n",
    "        for layer, svp in steering_vectors_positions_by_layer.items()\n",
    "    ],\n",
    "    suptitle=\"Steering Results Per Layer (STRICT):\\n'Steered' Answer Counts Summed Over All Prompts\",\n",
    ")\n",
    "\n",
    "create_bar_plot(\n",
    "    steering_counts_summed_over_landmarks_coeffs_strict,\n",
    "    layers,\n",
    "    \"Steering Results (STRICT):\\n'Steered' Answer Counts Summed Over All Prompts and Coeffs\",\n",
    "    \"Steering Vector (by layer #)\",\n",
    "    \"Count\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "landmarks = [pair[0] for pair in landmark_country_pairs]\n",
    "print(landmarks)\n",
    "\n",
    "# Strict steering results by across all layers.\n",
    "steering_counts_summed_over_layers_strict = steering_counts_all_strict.sum(\n",
    "    dim=0\n",
    ")  # shape (coeffs, landmarks)\n",
    "print(steering_counts_summed_over_layers_strict)\n",
    "\n",
    "# Strict steering results by across all layers and coeffs.\n",
    "steering_counts_summed_over_layers_coeffs_strict = (\n",
    "    steering_counts_summed_over_layers_strict.sum(dim=0)\n",
    ")  # shape (landmarks)\n",
    "print(steering_counts_summed_over_layers_coeffs_strict)\n",
    "\n",
    "create_sum_plot(\n",
    "    summed_counts=steering_counts_summed_over_layers_strict.T,\n",
    "    steering_coeffs=coeffs,\n",
    "    labels=landmarks,\n",
    "    suptitle=\"Steering Results Per Layer (STRICT):\\n'Steered' Answer Counts Summed Over All Layers\",\n",
    ")\n",
    "\n",
    "create_bar_plot(\n",
    "    steering_counts_summed_over_layers_coeffs_strict,\n",
    "    landmarks,\n",
    "    \"Steering Results (STRICT):\\n'Steered' Answer Counts Summed Over All Layers, and Coeffs\",\n",
    "    \"Landmark\",\n",
    "    \"Count\",\n",
    "    rotation=90,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Switch to Gemma 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "# gemma_2_2b = HookedTransformer.from_pretrained(\n",
    "#     \"gemma-2-2b\",\n",
    "#     device=DEVICE,\n",
    "# )\n",
    "\n",
    "# pprint(gemma_2_2b.cfg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Get Gemma 2 SAEs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "# width = 16\n",
    "\n",
    "# saes = []\n",
    "# for i in trange(gemma_2_2b.cfg.n_layers):\n",
    "#     print(f\"Downloading canonical SAE for layer {i} and width {width}k\")\n",
    "#     saes.append(\n",
    "#         SAE.from_pretrained(\n",
    "#             release=\"gemma-scope-2b-pt-res-canonical\",\n",
    "#             sae_id=f\"layer_{i}/width_{width}k/canonical\",\n",
    "#             device=DEVICE,\n",
    "#         )[0]\n",
    "#     )\n",
    "\n",
    "# pprint(saes[0].cfg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for sae in tqdm(saes):\n",
    "#     fpath = pathlib.Path(\n",
    "#         f\"./gemma-scope-2b-pt-res-canonical/layer_{sae.cfg.hook_layer}__width_{str(sae.cfg.d_sae)[:-3]}k\"\n",
    "#     )\n",
    "#     fpath.mkdir(parents=True, exist_ok=True)\n",
    "#     sae.save_model(fpath)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Find all features whose explanations contain keywords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "# EXPLANATIONS_GEMMA_FPATH = \"gemma-scope-2b-pt-res-canonical-w16k_explanations.json\"\n",
    "\n",
    "# try:\n",
    "#     with open(EXPLANATIONS_GEMMA_FPATH, \"r\") as f:\n",
    "#         explanations = json.load(f)\n",
    "# except FileNotFoundError:\n",
    "#     url = \"https://www.neuronpedia.org/api/explanation/export\"\n",
    "\n",
    "#     explanations = []\n",
    "\n",
    "#     for i in trange(len(saes)):\n",
    "#         sae = saes[i]\n",
    "#         model, sae_id = sae.cfg.neuronpedia_id.split(\"/\")\n",
    "\n",
    "#         querystring = {\"modelId\": model, \"saeId\": sae_id}\n",
    "\n",
    "#         headers = {\"X-Api-Key\": os.getenv(\"NEURONPEDIA_TOKEN\")}\n",
    "\n",
    "#         response = requests.get(url, headers=headers, params=querystring)\n",
    "\n",
    "#         explanations += response.json()\n",
    "\n",
    "#     with open(EXPLANATIONS_GEMMA_FPATH, \"w\") as f:\n",
    "#         json.dump(explanations, f, indent=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "# KEYWORDS = [\"POTTER\"]\n",
    "\n",
    "# explanations_filtered = [[] for i in range(len(saes))]\n",
    "# explanation_count = 0\n",
    "\n",
    "# for explanation in explanations:\n",
    "#     if any(keyword in explanation[\"description\"].upper() for keyword in KEYWORDS):\n",
    "#         layer = int(explanation[\"layer\"].split(\"-\")[0])\n",
    "#         explanations_filtered[layer].append(explanation)\n",
    "#         explanation_count += 1\n",
    "\n",
    "# for i in range(len(saes)):\n",
    "#     print(f\"Number of relevant features in layer {i}: {len(explanations_filtered[i])}\")\n",
    "\n",
    "# print(f\"Total relevant features: {explanation_count}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "# sv_prompt = \"Albus Dumbledore\"\n",
    "# sv_logits, cache = gemma_2_2b.run_with_cache(sv_prompt, prepend_bos=True)\n",
    "# tokens = gemma_2_2b.to_tokens(sv_prompt)\n",
    "# str_tokens = gemma_2_2b.to_str_tokens(tokens)\n",
    "# print(f\"Tokens: {tokens}\")\n",
    "# print(f\"Token strings: {str_tokens}\")\n",
    "\n",
    "# k = 6\n",
    "# act_threshold_relative = 0.005\n",
    "\n",
    "# saes_out = []\n",
    "# sv_feature_acts_vals_sorted_all_layers = []\n",
    "# sv_feature_acts_idx_sorted_all_layers = []\n",
    "\n",
    "# for i, sae in enumerate(saes):\n",
    "#     explanations_filtered_idx = t.tensor(\n",
    "#         [int(explanation[\"index\"]) for explanation in explanations_filtered[i]],\n",
    "#         device=DEVICE,\n",
    "#     )\n",
    "#     if explanations_filtered_idx.numel() == 0:\n",
    "#         continue\n",
    "\n",
    "#     sv_feature_acts = sae.encode(cache[sae.cfg.hook_name])\n",
    "#     saes_out.append(sae.decode(sv_feature_acts))\n",
    "\n",
    "#     act_max = sv_feature_acts.max()\n",
    "#     act_threshold = act_threshold_relative * act_max\n",
    "\n",
    "#     sv_feature_acts_filtered = sv_feature_acts[:, :, explanations_filtered_idx]\n",
    "\n",
    "#     if (\n",
    "#         sv_feature_acts_filtered.sum() * sv_feature_acts_filtered.numel()\n",
    "#         < act_threshold\n",
    "#     ):\n",
    "#         continue\n",
    "\n",
    "#     sv_feature_acts_vals_sorted, sv_feature_acts_idx_sorted = (\n",
    "#         sv_feature_acts_filtered.sort(descending=True, dim=-1)\n",
    "#     )\n",
    "\n",
    "#     print(f\"\\nSorted activations for layer {i}\")\n",
    "#     print(f\"\\tMax activation for layer {i}: {act_max}\")\n",
    "#     for token_idx, str_token in enumerate(str_tokens):\n",
    "#         vals_for_token = sv_feature_acts_vals_sorted[0, token_idx]\n",
    "#         idx_for_token = explanations_filtered_idx[sv_feature_acts_idx_sorted][\n",
    "#             0, token_idx\n",
    "#         ]\n",
    "#         mask = vals_for_token >= act_threshold\n",
    "#         if not mask.any():\n",
    "#             continue\n",
    "#         print(f\"\\tToken {token_idx}: '{str_token}'\")\n",
    "#         print(f\"\\t\\tRelevant feature activations: {vals_for_token[mask].tolist()}\")\n",
    "#         print(f\"\\t\\tRelevant feature indices: {idx_for_token[mask].tolist()}\")\n",
    "\n",
    "# # from sae_lens.analysis.neuronpedia_integration import get_neuronpedia_quick_list\n",
    "\n",
    "# # get_neuronpedia_quick_list(\n",
    "# #     sae=sae, features=sv_feature_acts_idx[:, :, :].flatten().tolist()\n",
    "# # )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "# STEERING_LAYER = 8\n",
    "# steering_vector = saes[STEERING_LAYER].W_dec[3012]\n",
    "\n",
    "# example_prompt = \"My favourite protagonist in any fantasy novel series is named\"\n",
    "# coeff = 300\n",
    "# sampling_kwargs = dict(temperature=1.0, top_p=0.1, freq_penalty=1.0)\n",
    "\n",
    "# model = gemma_2_2b\n",
    "\n",
    "# sae_out = saes_out[STEERING_LAYER]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def steering_hook(resid_pre, hook):\n",
    "#     if resid_pre.shape[1] == 1:\n",
    "#         return\n",
    "\n",
    "#     position = sae_out.shape[1]\n",
    "#     if steering_on:\n",
    "#         # using our steering vector and applying the coefficient\n",
    "#         resid_pre[:, : position - 1, :] += coeff * steering_vector\n",
    "\n",
    "\n",
    "# def hooked_generate(prompt_batch, fwd_hooks=[], seed=None, **kwargs):\n",
    "#     if seed is not None:\n",
    "#         t.manual_seed(seed)\n",
    "\n",
    "#     with model.hooks(fwd_hooks=fwd_hooks):\n",
    "#         tokenized = model.to_tokens(prompt_batch)\n",
    "#         result = model.generate(\n",
    "#             stop_at_eos=False,  # avoids a bug on MPS\n",
    "#             input=tokenized,\n",
    "#             max_new_tokens=50,\n",
    "#             do_sample=True,\n",
    "#             **kwargs,\n",
    "#         )\n",
    "#     return result\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def run_generate(example_prompt):\n",
    "#     model.reset_hooks()\n",
    "#     editing_hooks = [(f\"blocks.{STEERING_LAYER}.hook_resid_post\", steering_hook)]\n",
    "#     res = hooked_generate(\n",
    "#         [example_prompt] * 3, editing_hooks, seed=None, **sampling_kwargs\n",
    "#     )\n",
    "\n",
    "#     # Print results, removing the ugly beginning of sequence token\n",
    "#     res_str = model.to_string(res[:, 1:])\n",
    "#     print((\"\\n\" + \"-\" * 80 + \"\\n\").join(res_str))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "# steering_on = True\n",
    "# run_generate(example_prompt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "# steering_on = False\n",
    "# run_generate(example_prompt)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
